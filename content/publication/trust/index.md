---
title: "Can Large Language Model Agents Simulate Human Trust Behaviors?"
authors:
- admin
- Canyu Chen
- Feiran Jia
- Ziyu Ye
- Shiyang Lai
- Kai Shu
- Jindong Gu
- Adel Bibi
- Ziniu Hu
- David Jurgens
- James Evans
- Philip H.S. Torr
- Bernard Ghanem
- Guohao Li
date: "2024-10-05T00:00:00Z"
doi: ""

# Schedule page publish date (NOT publication's date).
publishDate: "2024-10-05T00:00:00Z"

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["1"]

# Publication name and optional abbreviated publication name.
publication: "<span style='color:red;'>Accepted at NeurIPS 2024</span>"
publication_short: "<span style='color:red;'>Accepted at NeurIPS 2024</span>"


abstract: "Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents reliably simulate human behavior? In this paper, we focus on a key aspect of human interaction: trust, and aim to investigate whether LLM agents can effectively simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, which we refer to as agent trust, under the framework of Trust Games, a widely recognized tool in behavioral economics. Additionally, we discover that GPT-4 agents demonstrate high behavioral alignment with humans in terms of trust behaviors, suggesting the feasibility of simulating human trust behaviors with LLM agents. Moreover, we investigate the biases in agent trust and the differences in trust directed towards other LLM agents versus humans. We also examine the intrinsic properties of agent trust under conditions such as advanced reasoning strategies and external manipulations. Our study provides new insights into the behaviors of LLM agents and highlights the fundamental analogy between LLMs and humans beyond value alignment. We further discuss the broad implications of our findings for various applications where trust plays a critical role."

# Summary. An optional shortened abstract.

tags:
- Source Themes
featured: false

links:
#- name: Custom Link
#  url: #
url_pdf: https://arxiv.org/abs/2402.04559
url_code: https://github.com/camel-ai/agent-trust
#url_dataset: '#'
#url_poster: '#'
url_project: https://github.com/camel-ai/agent-trust
#url_slides: '#'
#url_source: '#'
#url_video: '#'

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
#image:
#  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/s9CC2SKySJM)'
#  focal_point: ""
#  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects:
- research-project

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
# slides: example
---

<!-- {{% callout note %}}
Create your slides in Markdown - click the *Slides* button to check out the example.
{{% /callout %}} -->

<!-- At present, the paper has not yet been published, and relevant materials will not be displayed. -->
Our project URL: https://llm-agent-trust-behavior.github.io/