---
title: Audio Clip
summary: Using Audio and Text do Clip training.(SenseTime Intern)
tags:
  - Deep Learning
  - intern
date: '2022-04-27T00:00:00Z'
# Optional external URL for project (replaces project detail page).
external_link: ''

image:
  caption: Photo by rawpixel on Unsplash
  focal_point: Smart
url_code: 'https://github.com/yitianlian/audio-text-clip'
url_pdf: ''
url_slides: ''
url_video: ''

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: example
---

In recent years, the development of multimodal learning models such as OpenAI's CLIP has significantly improved the understanding and generation of images and their associated textual descriptions. Inspired by the success of CLIP in the vision-language domain, we propose a novel approach called Contrastive Language-Audio Pretraining, which aims to learn the relationship between audio data and textual descriptions.

In this project, we replace the image data in the CLIP model with audio data and train the model using a large dataset of audio clips with corresponding textual descriptions. The objective is to develop a multimodal model that can effectively understand and generate audio and their associated textual descriptions.
